{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b6yvqOcf0cQ",
        "outputId": "dd253763-743f-4548-9a39-243d859282f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   other_topic  \\\n",
            "0        Read moore books, read less facebook.   \n",
            "1                                          NaN   \n",
            "2                                          NaN   \n",
            "3  Help More\\nspread pet therapy|helping other   \n",
            "4                                          NaN   \n",
            "\n",
            "                                   resolution_topics  gender             name  \\\n",
            "0                                      Eat healthier  female      Dena_Marina   \n",
            "1  Humor about Personal Growth and Interests Reso...  female     ninjagirl325   \n",
            "2                                  Be More Confident    male     RickyDelReyy   \n",
            "3                                              Other    male        CalmareNJ   \n",
            "4                                   Be more positive  female  welovatoyoudemi   \n",
            "\n",
            "  Resolution_Category  retweet_count  \\\n",
            "0    Health & Fitness            0.0   \n",
            "1               Humor            1.0   \n",
            "2     Personal Growth            0.0   \n",
            "3       Philanthropic            0.0   \n",
            "4     Personal Growth            0.0   \n",
            "\n",
            "                                               tweet tweet_coord  \\\n",
            "0  #NewYearsResolution :: Read more books, No scr...         NaN   \n",
            "1  #NewYearsResolution Finally master @ZJ10 's pa...         NaN   \n",
            "2  #NewYearsResolution to stop being so damn perf...         NaN   \n",
            "3  My #NewYearsResolution is to help my disabled ...         NaN   \n",
            "4  #NewYearsResolution #2015Goals #2015bucketlist...         NaN   \n",
            "\n",
            "    tweet_created tweet_date      tweet_id             tweet_location  \\\n",
            "0  12/31/14 10:48   12/31/14  5.500000e+17        Southern California   \n",
            "1  12/31/14 10:47   12/31/14  5.500000e+17                 New Jersey   \n",
            "2  12/31/14 10:46   12/31/14  5.500000e+17                  Hollywood   \n",
            "3  12/31/14 10:45   12/31/14  5.500000e+17                  Metro NYC   \n",
            "4  12/31/14 10:44   12/31/14  5.500000e+17  Pittsburgh, Pennsylvania    \n",
            "\n",
            "  tweet_state               user_timezone tweet_region  \n",
            "0          CA  Pacific Time (US & Canada)         West  \n",
            "1          NJ  Central Time (US & Canada)    Northeast  \n",
            "2          CA  Eastern Time (US & Canada)         West  \n",
            "3          NY                         NaN    Northeast  \n",
            "4          PA  Eastern Time (US & Canada)    Northeast  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                               tweet  Positive  Negative  \\\n",
            "0  newyearsresolut  read book scroll fbcheck emai...     0.000      0.00   \n",
            "1    newyearsresolut final master  part kitchen sink     0.000      0.00   \n",
            "2              newyearsresolut stop damn perf √π√µ√∞√π√µ√©     0.000      0.62   \n",
            "3  newyearsresolut help disabl patient discov emo...     0.434      0.00   \n",
            "4                  newyearsresolut   continued¬â¬ù¬ï√º¬è      0.000      0.00   \n",
            "\n",
            "   Neutral  \n",
            "0    1.000  \n",
            "1    1.000  \n",
            "2    0.380  \n",
            "3    0.566  \n",
            "4    1.000  \n",
            "Neutral üôÇ \n",
            "Positive:  641.8720000000008\n",
            "Negative:  369.0070000000004\n",
            "Neutral:  4000.130999999994\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "data = pd.read_csv('newyears.csv',on_bad_lines='skip',encoding = 'unicode_escape')\n",
        "print(data.head())\n",
        "nltk.download('stopwords')\n",
        "stemmer = nltk.SnowballStemmer(\"english\")\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "stopword=set(stopwords.words('english'))\n",
        "\n",
        "def clean(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = [word for word in text.split(' ') if word not in stopword]\n",
        "    text=\" \".join(text)\n",
        "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
        "    text=\" \".join(text)\n",
        "    return text\n",
        "data[\"tweet\"] = data[\"tweet\"].apply(clean)\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sentiments = SentimentIntensityAnalyzer()\n",
        "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"tweet\"]]\n",
        "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"tweet\"]]\n",
        "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"tweet\"]]\n",
        "data = data[[\"tweet\", \"Positive\",\n",
        "             \"Negative\", \"Neutral\"]]\n",
        "print(data.head())\n",
        "x = sum(data[\"Positive\"])\n",
        "y = sum(data[\"Negative\"])\n",
        "z = sum(data[\"Neutral\"])\n",
        "\n",
        "def sentiment_score(a, b, c):\n",
        "    if (a>b) and (a>c):\n",
        "        print(\"Positive üòä \")\n",
        "    elif (b>a) and (b>c):\n",
        "        print(\"Negative üò† \")\n",
        "    else:\n",
        "        print(\"Neutral üôÇ \")\n",
        "sentiment_score(x, y, z)\n",
        "print(\"Positive: \", x)\n",
        "print(\"Negative: \", y)\n",
        "print(\"Neutral: \", z)\n"
      ]
    }
  ]
}